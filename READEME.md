# ğŸ¤– RAG Assistant - LangChain & Ollama

Este projeto Ã© uma implementaÃ§Ã£o modular de um sistema **RAG (Retrieval-Augmented Generation)** utilizando **LangChain** e **Ollama**. A aplicaÃ§Ã£o permite interagir via terminal com uma base de conhecimento formada por documentos PDF, utilizando modelos de LLM rodando localmente.

O projeto foi refatorado seguindo princÃ­pios de **Engenharia de Software**, focando em separaÃ§Ã£o de responsabilidades (SoC), manutenibilidade e eficiÃªncia.

## âœ¨ Funcionalidades

* **RAG Local:** Utiliza modelos open-source rodando na sua prÃ³pria mÃ¡quina via Ollama (sem custos de API).
* **Arquitetura Modular:** CÃ³digo desacoplado em mÃ³dulos de ingestÃ£o, banco vetorial, configuraÃ§Ã£o e pipeline de IA.
* **PersistÃªncia de Vetores:** O Ã­ndice FAISS Ã© salvo em disco (`faiss_index/`). O processamento pesado dos documentos sÃ³ ocorre na primeira execuÃ§Ã£o; nas seguintes, o carregamento Ã© instantÃ¢neo.
* **Streaming de Resposta:** A resposta Ã© gerada token por token no terminal, proporcionando uma experiÃªncia de chat fluida.
* **Observabilidade (Opcional):** IntegraÃ§Ã£o pronta para o **LangSmith** para tracing e debugging das cadeias de execuÃ§Ã£o.

## ğŸ› ï¸ Tecnologias

* **Linguagem:** Python 3.10+
* **OrquestraÃ§Ã£o:** LangChain
* **LLM & Embeddings:** Ollama (Gemma 3 & BGE-M3)
* **Vector Store:** FAISS
* **TokenizaÃ§Ã£o:** HuggingFace Transformers

## âš™ï¸ PrÃ©-requisitos

1. Ter o [Python](https://www.python.org/) instalado.
2. Ter o [Ollama](https://ollama.com/) instalado e rodando.
3. Baixar os modelos necessÃ¡rios via terminal:

```bash
# Modelo de LLM (Gemma 3 - 4B)
ollama pull gemma3:4b

# Modelo de Embeddings (BGE-M3)
ollama pull bge-m3
```

> **Nota:** Se desejar alterar os modelos, edite o arquivo `src/config.py` ou utilize variÃ¡veis de ambiente.

## ğŸš€ Como Rodar o Projeto

### 1. Clone o repositÃ³rio
```bash
git clone [https://github.com/seu-usuario/seu-repositorio.git](https://github.com/seu-usuario/seu-repositorio.git)
cd seu-repositorio
```

### 2. Crie e ative o ambiente virtual
```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

### 3. Instale as dependÃªncias
```bash
pip install -r requirements.txt
```

### 4. ConfiguraÃ§Ã£o do LangSmith (Opcional)
Para monitorar o funcionamento interno da IA, crie um arquivo `.env` na raiz do projeto com as chaves abaixo. Se nÃ£o quiser usar, o projeto rodarÃ¡ normalmente sem ele.

```ini
# Arquivo .env
LANGCHAIN_TRACING_V2=true
LANGCHAIN_ENDPOINT="[https://api.smith.langchain.com](https://api.smith.langchain.com)"
LANGCHAIN_API_KEY="sua-api-key-aqui"
LANGCHAIN_PROJECT="rag-local-ollama"
```

### 5. Adicione seus documentos
Coloque os arquivos PDF que deseja consultar dentro da pasta:
```
documents/
```

### 6. Execute a aplicaÃ§Ã£o
```bash
python main.py
```

## ğŸ“‚ Estrutura do Projeto

```text
.
â”œâ”€â”€ documents/          # Coloque seus PDFs aqui
â”œâ”€â”€ faiss_index/        # Banco vetorial salvo (gerado automaticamente)
â”œâ”€â”€ src/                # CÃ³digo fonte
â”‚   â”œâ”€â”€ config.py       # ConfiguraÃ§Ãµes globais
â”‚   â”œâ”€â”€ ingestor.py     # Leitura e chunking de PDFs
â”‚   â”œâ”€â”€ vector_db.py    # Gerenciamento do Ã­ndice FAISS
â”‚   â””â”€â”€ rag_chain.py    # DefiniÃ§Ã£o do Prompt e Chain
â”œâ”€â”€ main.py             # Arquivo principal (CLI)
â”œâ”€â”€ requirements.txt    # DependÃªncias
â”œâ”€â”€ .env                # VariÃ¡veis de ambiente (nÃ£o versionado)
â””â”€â”€ README.md           # DocumentaÃ§Ã£o
```

## ğŸ§¹ Atualizando a Base de Conhecimento

O sistema verifica se a pasta `faiss_index/` existe para carregar o banco rapidamente. 

**Para adicionar ou remover documentos:**
1. Adicione/remova os PDFs na pasta `documents/`.
2. Apague a pasta `faiss_index/`.
3. Rode o `python main.py` novamente. O sistema irÃ¡ reprocessar os arquivos e criar um Ã­ndice atualizado.

## ğŸ¤ ContribuiÃ§Ã£o

Sinta-se Ã  vontade para abrir issues e pull requests para melhorias no cÃ³digo ou na documentaÃ§Ã£o.